# Repo_Based_Interviewer

A RAG (Retrieval-Augmented Generation) based intelligent interviewer system that analyzes GitHub repositories and conducts interactive technical interviews based on the repository content.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [How It Works](#how-it-works)
- [Project Structure](#project-structure)
- [Technologies Used](#technologies-used)
- [Example Workflow](#example-workflow)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

**Repo_Based_Interviewer** is an AI-powered system designed to conduct technical interviews based on the content of GitHub repositories. By leveraging RAG (Retrieval-Augmented Generation) technology, the system can:

- **Analyze** any public GitHub repository
- **Generate** relevant technical questions based on the codebase
- **Evaluate** user responses to assess technical understanding
- **Provide** feedback on answers with context from the repository

This tool is ideal for:
- Technical recruiters conducting code-based interviews
- Developers wanting to test their understanding of a codebase
- Educational institutions for assessment purposes
- Teams onboarding new developers

## âœ¨ Features

### Core Capabilities

- **Repository Analysis**: Automatically clones and analyzes GitHub repositories
- **Intelligent Question Generation**: Creates context-aware questions from:
  - Code structure and patterns
  - Documentation and comments
  - Dependencies and configurations
  - Architecture and design patterns
  
- **Multi-Level Questioning**: Generates questions at various difficulty levels:
  - Basic: Understanding of repository structure and purpose
  - Intermediate: Code implementation details
  - Advanced: Architecture decisions and optimization strategies

- **Answer Evaluation**: Uses RAG to:
  - Compare user answers against repository context
  - Provide detailed feedback
  - Score responses based on accuracy and completeness

- **Interactive Interview Sessions**: 
  - Real-time question-answer flow
  - Follow-up questions based on responses
  - Progress tracking throughout the interview

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Interface                           â”‚
â”‚              (CLI / Web Interface)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Input Handler                              â”‚
â”‚         (GitHub URL Parser & Validator)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Repository Processor                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚  Repo Cloner     â”‚    â”‚  Code Parser     â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RAG System Core                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚ Vector Database  â”‚    â”‚  Embeddings      â”‚           â”‚
â”‚    â”‚   (ChromaDB/     â”‚â—„â”€â”€â”€â”‚  Generator       â”‚           â”‚
â”‚    â”‚    Pinecone)     â”‚    â”‚  (OpenAI/HF)     â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Question Generation Engine                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚ Context Retrievalâ”‚    â”‚  LLM Integration â”‚           â”‚
â”‚    â”‚                  â”‚â”€â”€â”€â–ºâ”‚  (GPT-4/Claude)  â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Answer Evaluation System                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚ Semantic Matcher â”‚    â”‚  Scoring Engine  â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Results & Feedback                             â”‚
â”‚         (Report Generation & Analytics)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- Git
- API keys for LLM service (OpenAI, Anthropic, or Hugging Face)
- Sufficient disk space for cloning repositories

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/Mageshwaran18/Repo_Based_Interviewer.git
   cd Repo_Based_Interviewer
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys and settings
   ```

5. **Initialize the vector database**
   ```bash
   python setup.py init-db
   ```

## ğŸ’» Usage

### Basic Command Line Interface

```bash
# Start an interview session with a GitHub repository
python main.py --repo https://github.com/user/repository

# Specify question difficulty level
python main.py --repo https://github.com/user/repository --level intermediate

# Set number of questions
python main.py --repo https://github.com/user/repository --questions 10

# Generate a detailed report
python main.py --repo https://github.com/user/repository --report output.pdf
```

### Interactive Python API

```python
from repo_interviewer import RepoInterviewer

# Initialize the interviewer
interviewer = RepoInterviewer(
    repo_url="https://github.com/user/repository",
    api_key="your-api-key"
)

# Analyze the repository
interviewer.analyze_repository()

# Start interview session
session = interviewer.start_interview(
    num_questions=5,
    difficulty="intermediate"
)

# Get a question
question = session.get_next_question()
print(f"Q: {question.text}")

# Submit an answer
user_answer = input("Your answer: ")
evaluation = session.evaluate_answer(user_answer)

print(f"Score: {evaluation.score}/10")
print(f"Feedback: {evaluation.feedback}")

# Complete the interview
report = session.generate_report()
report.save("interview_report.pdf")
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
# LLM Configuration
LLM_PROVIDER=openai  # options: openai, anthropic, huggingface
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
HUGGINGFACE_API_KEY=your_hf_api_key

# Vector Database
VECTOR_DB=chromadb  # options: chromadb, pinecone, faiss
CHROMADB_PATH=./data/chromadb
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=us-west1-gcp

# Repository Settings
TEMP_REPO_PATH=./temp/repos
MAX_REPO_SIZE_MB=500
ALLOWED_LANGUAGES=python,javascript,java,go,rust,typescript

# Question Generation
DEFAULT_NUM_QUESTIONS=5
MIN_DIFFICULTY=1
MAX_DIFFICULTY=10
QUESTION_TYPES=code_understanding,architecture,debugging,best_practices

# Evaluation
SIMILARITY_THRESHOLD=0.7
SCORING_METHOD=semantic  # options: semantic, keyword, hybrid

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/interviewer.log
```

### Configuration File

Alternatively, use `config.yaml`:

```yaml
llm:
  provider: openai
  model: gpt-4
  temperature: 0.7
  max_tokens: 2000

vector_db:
  type: chromadb
  persist_directory: ./data/chromadb
  collection_name: repo_embeddings

repository:
  temp_path: ./temp/repos
  max_size_mb: 500
  clone_depth: 1
  ignored_paths:
    - node_modules
    - venv
    - .git
    - __pycache__

interview:
  default_questions: 5
  difficulty_levels:
    - basic
    - intermediate
    - advanced
  question_categories:
    - code_structure
    - implementation_details
    - design_patterns
    - best_practices
    - debugging

evaluation:
  scoring:
    semantic_weight: 0.6
    keyword_weight: 0.4
  thresholds:
    excellent: 0.9
    good: 0.7
    satisfactory: 0.5
```

## ğŸ” How It Works

### 1. Repository Analysis Phase

When you provide a GitHub repository URL, the system:

1. **Clones the repository** to a temporary local directory
2. **Scans the codebase** to identify:
   - Programming languages used
   - Project structure and organization
   - Key files (README, configuration, main source files)
   - Dependencies and external libraries
3. **Parses code files** to extract:
   - Function and class definitions
   - Comments and docstrings
   - Import statements and dependencies
   - Code patterns and conventions

### 2. Embedding Generation

The system processes the repository content:

1. **Chunks the content** into meaningful segments:
   - Individual functions/methods
   - Classes and modules
   - Documentation sections
   - Configuration files

2. **Generates embeddings** using the configured LLM:
   - Converts text to vector representations
   - Stores embeddings in the vector database
   - Indexes for efficient retrieval

### 3. Question Generation

Based on the analyzed repository:

1. **Retrieves relevant context** from the vector database
2. **Generates questions** using the LLM:
   - Formulates questions about code functionality
   - Creates scenario-based questions
   - Asks about design decisions and patterns
3. **Categorizes questions** by difficulty and type

### 4. Interview Execution

During the interview:

1. **Presents questions** one at a time
2. **Captures user responses**
3. **Allows time for thoughtful answers**
4. **Supports follow-up questions** based on responses

### 5. Answer Evaluation

For each answer:

1. **Retrieves relevant context** from the repository
2. **Compares answer** with expected knowledge:
   - Semantic similarity matching
   - Keyword extraction and matching
   - Contextual relevance scoring
3. **Generates detailed feedback**:
   - What was correct
   - What was missing
   - Additional insights from the codebase

### 6. Report Generation

After the interview:

1. **Compiles results** from all questions
2. **Calculates overall score**
3. **Generates insights**:
   - Strengths demonstrated
   - Areas for improvement
   - Repository comprehension level
4. **Exports report** in various formats (PDF, JSON, HTML)

## ğŸ“ Project Structure

```
Repo_Based_Interviewer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # Entry point
â”‚   â”œâ”€â”€ cli.py                     # Command-line interface
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py            # Configuration management
â”‚   â”‚   â””â”€â”€ config.yaml            # Default configuration
â”‚   â”œâ”€â”€ repository/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cloner.py              # Repository cloning logic
â”‚   â”‚   â”œâ”€â”€ parser.py              # Code parsing utilities
â”‚   â”‚   â””â”€â”€ analyzer.py            # Repository analysis
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py          # Embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py        # Vector database interface
â”‚   â”‚   â””â”€â”€ retriever.py           # Context retrieval
â”‚   â”œâ”€â”€ question/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py           # Question generation
â”‚   â”‚   â”œâ”€â”€ categorizer.py         # Question categorization
â”‚   â”‚   â””â”€â”€ templates.py           # Question templates
â”‚   â”œâ”€â”€ interview/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ session.py             # Interview session management
â”‚   â”‚   â”œâ”€â”€ evaluator.py           # Answer evaluation
â”‚   â”‚   â””â”€â”€ scorer.py              # Scoring logic
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                # Base LLM interface
â”‚   â”‚   â”œâ”€â”€ openai_client.py      # OpenAI integration
â”‚   â”‚   â”œâ”€â”€ anthropic_client.py   # Anthropic integration
â”‚   â”‚   â””â”€â”€ huggingface_client.py # Hugging Face integration
â”‚   â”œâ”€â”€ report/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py           # Report generation
â”‚   â”‚   â””â”€â”€ templates/             # Report templates
â”‚   â”‚       â”œâ”€â”€ pdf_template.html
â”‚   â”‚       â””â”€â”€ html_template.html
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py              # Logging utilities
â”‚       â”œâ”€â”€ validators.py          # Input validation
â”‚       â””â”€â”€ helpers.py             # Helper functions
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_repository.py
â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”œâ”€â”€ test_questions.py
â”‚   â”œâ”€â”€ test_interview.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chromadb/                  # Vector database storage
â”‚   â””â”€â”€ cache/                     # Cached embeddings
â”œâ”€â”€ temp/
â”‚   â””â”€â”€ repos/                     # Temporary repository clones
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ interviewer.log            # Application logs
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ advanced_usage.py
â”‚   â””â”€â”€ custom_questions.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â”œâ”€â”€ deployment.md
â”‚   â””â”€â”€ troubleshooting.md
â”œâ”€â”€ .env.example                   # Example environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ setup.py                       # Package setup
â”œâ”€â”€ pyproject.toml                 # Project metadata
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ LICENSE
```

## ğŸ› ï¸ Technologies Used

### Core Technologies

- **Python 3.8+**: Primary programming language
- **LangChain**: Framework for LLM application development
- **OpenAI API / Anthropic / Hugging Face**: Large Language Models for question generation and evaluation

### RAG Components

- **ChromaDB / Pinecone / FAISS**: Vector database for embeddings storage
- **Sentence Transformers**: Generate semantic embeddings
- **tiktoken**: Token counting and text chunking

### Repository Analysis

- **GitPython**: Git repository interaction
- **tree-sitter**: Code parsing for multiple languages
- **pygments**: Syntax highlighting and language detection

### Additional Libraries

- **python-dotenv**: Environment variable management
- **pydantic**: Data validation and settings management
- **rich**: Enhanced terminal output
- **typer**: CLI framework
- **pytest**: Testing framework
- **reportlab / weasyprint**: PDF report generation

## ğŸ“ Example Workflow

### Example 1: Basic Interview

```bash
# Start a basic interview
$ python main.py --repo https://github.com/pallets/flask

Analyzing repository: flask...
âœ“ Repository cloned successfully
âœ“ Found 152 Python files
âœ“ Generated embeddings for 1,234 code chunks
âœ“ Ready to start interview

Starting interview session...

Question 1/5 [Difficulty: Intermediate]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
What is the primary purpose of the Flask class in the main 
application module, and what design pattern does it implement?

Your answer: [User provides answer]

Evaluation:
Score: 8/10
âœ“ Correctly identified Flask as the main application class
âœ“ Mentioned WSGI application pattern
âœ— Could have mentioned the factory pattern usage
âœ“ Good understanding of core concepts

Feedback: Your answer demonstrates solid understanding of Flask's 
architecture. The Flask class indeed serves as the central WSGI 
application. Consider exploring the application factory pattern 
used in larger Flask applications for improved modularity.

[Continue with remaining questions...]

Interview Complete!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Overall Score: 42/50 (84%)

Strengths:
âœ“ Strong understanding of core concepts
âœ“ Good grasp of Flask's routing mechanism
âœ“ Understood decorator patterns

Areas for Improvement:
â€¢ Advanced configuration management
â€¢ Testing strategies
â€¢ Application factory pattern

Report saved to: interview_report_flask_20241116.pdf
```

### Example 2: Programmatic Usage

```python
from repo_interviewer import RepoInterviewer

# Initialize with custom settings
interviewer = RepoInterviewer(
    repo_url="https://github.com/django/django",
    config={
        "llm_provider": "openai",
        "model": "gpt-4",
        "num_questions": 10,
        "difficulty_range": (5, 8),  # Medium to Hard
        "focus_areas": ["orm", "views", "middleware"]
    }
)

# Analyze repository
analysis = interviewer.analyze_repository()
print(f"Detected languages: {analysis.languages}")
print(f"Total files: {analysis.file_count}")
print(f"Lines of code: {analysis.loc}")

# Generate custom questions
questions = interviewer.generate_questions(
    topics=["database", "authentication"],
    num_per_topic=3
)

# Start interview
session = interviewer.start_interview(questions=questions)

for i, question in enumerate(session.questions, 1):
    print(f"\n--- Question {i} ---")
    print(question.text)
    
    # Get user answer (could be from UI, API, etc.)
    user_answer = get_user_input()
    
    # Evaluate answer
    result = session.evaluate_answer(question.id, user_answer)
    
    print(f"Score: {result.score}")
    print(f"Feedback: {result.feedback}")
    
    # Ask follow-up if needed
    if result.score < 7 and question.has_followup:
        followup = session.generate_followup(question.id, user_answer)
        print(f"Follow-up: {followup.text}")

# Generate comprehensive report
report = session.generate_report(
    include_answers=True,
    include_code_references=True,
    format="pdf"
)

report.save("django_interview_detailed.pdf")
```

## ğŸ¤ Contributing

We welcome contributions to the Repo_Based_Interviewer project! Here's how you can help:

### Ways to Contribute

1. **Report Bugs**: Open an issue describing the bug and how to reproduce it
2. **Suggest Features**: Share ideas for new features or improvements
3. **Submit Pull Requests**: Fix bugs or implement new features
4. **Improve Documentation**: Help us make the docs clearer and more comprehensive
5. **Share Feedback**: Let us know how you're using the tool

### Development Setup

```bash
# Fork and clone the repository
git clone https://github.com/your-username/Repo_Based_Interviewer.git
cd Repo_Based_Interviewer

# Create a development branch
git checkout -b feature/your-feature-name

# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest

# Run linting
flake8 src/
black src/ --check
mypy src/

# Make your changes and commit
git add .
git commit -m "Description of changes"

# Push and create a pull request
git push origin feature/your-feature-name
```

### Code Standards

- Follow PEP 8 style guidelines
- Write comprehensive docstrings
- Add unit tests for new features
- Maintain backward compatibility
- Update documentation as needed

### Pull Request Process

1. Ensure all tests pass
2. Update the README if needed
3. Add your changes to CHANGELOG.md
4. Request review from maintainers
5. Address review feedback

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [LangChain](https://www.langchain.com/) framework
- Vector database powered by [ChromaDB](https://www.trychroma.com/)
- LLM services from [OpenAI](https://openai.com/), [Anthropic](https://www.anthropic.com/), and [Hugging Face](https://huggingface.co/)
- Inspired by the need for better technical assessment tools

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/Mageshwaran18/Repo_Based_Interviewer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Mageshwaran18/Repo_Based_Interviewer/discussions)
- **Email**: [support@repo-interviewer.dev](mailto:support@repo-interviewer.dev)

## ğŸ—ºï¸ Roadmap

### Current Version (v1.0)
- âœ… Basic repository analysis
- âœ… Question generation from code
- âœ… Answer evaluation
- âœ… Report generation

### Upcoming Features (v1.1)
- [ ] Support for private repositories
- [ ] Multi-language support (beyond English)
- [ ] Video interview mode with speech-to-text
- [ ] Integration with popular ATS systems
- [ ] Team collaboration features

### Future Plans (v2.0)
- [ ] Real-time collaborative interviews
- [ ] AI-powered code review feedback
- [ ] Custom question banks
- [ ] Analytics dashboard
- [ ] Mobile application

---

**Made with â¤ï¸ by the Repo_Based_Interviewer Team**

*Star â­ this repository if you find it helpful!*