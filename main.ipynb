{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7089c679",
   "metadata": {},
   "source": [
    "# Download valid files from the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96df15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\RBI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import io\n",
    "from docx import Document \n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import List\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import List\n",
    "import nbformat\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2172e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_PATH  = \"bm25_encoder.json\"  # Path to save/load BM25 encoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb60af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_EXTENSIONS = {\n",
    "    \".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".svg\", \".webp\",\n",
    "    \".ico\", \".tiff\", \".tif\", \".mp3\", \".mp4\", \".wav\", \".avi\",\n",
    "    \".mov\", \".zip\", \".tar\", \".gz\", \".rar\", \".7z\", \".pdf\",\n",
    "    \".exe\", \".dll\", \".so\", \".csv\", \".tsv\"\n",
    "}\n",
    "\n",
    "def download_github_repo(repo_url, save_dir=\"data/repo_files\"):\n",
    "    # remove .git if present\n",
    "    repo_url = repo_url.replace(\".git\", \"\")\n",
    "\n",
    "    match = re.match(r\"https://github.com/([^/]+)/([^/]+)\", repo_url)\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid GitHub URL\")\n",
    "\n",
    "    user, repo = match.groups()\n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def download_recursive(api_path, local_path):\n",
    "        response = requests.get(api_path)\n",
    "        data = response.json()\n",
    "\n",
    "        # handle API errors (rate limit, not found, etc)\n",
    "        if isinstance(data, dict) and \"message\" in data:\n",
    "            print(\"❌ GitHub API Error:\", data[\"message\"])\n",
    "            return\n",
    "\n",
    "        for item in data:\n",
    "            name = item[\"name\"]\n",
    "            file_path = os.path.join(local_path, name)\n",
    "\n",
    "            # skip unwanted folders\n",
    "            if name.lower() in [\"node_modules\", \".git\"]:\n",
    "                continue\n",
    "\n",
    "            if item[\"type\"] == \"dir\":\n",
    "                os.makedirs(file_path, exist_ok=True)\n",
    "                download_recursive(item[\"url\"], file_path)\n",
    "\n",
    "            elif item[\"type\"] == \"file\":\n",
    "                ext = os.path.splitext(name)[1].lower()\n",
    "                if ext in SKIP_EXTENSIONS:\n",
    "                    continue\n",
    "\n",
    "                file_data = requests.get(item[\"download_url\"]).content\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(file_data)\n",
    "\n",
    "                print(\"Downloaded:\", file_path)\n",
    "\n",
    "    download_recursive(api_url, save_dir)\n",
    "    print(\"\\n✔ Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef45606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: data/repo_files\\Music_Popularity_Prediction.ipynb\n",
      "Downloaded: data/repo_files\\README.md\n",
      "Downloaded: data/repo_files\\requirements.txt\n",
      "\n",
      "✔ Download complete!\n"
     ]
    }
   ],
   "source": [
    "download_github_repo(\"https://github.com/Mageshwaran18/Music_Popularity_Prediction.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc62ce",
   "metadata": {},
   "source": [
    "# Convert those downloaded files to a common txt format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810416ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EXTENSIONS = {\".txt\", \".py\", \".md\", \".json\", \".csv\", \".yaml\", \".yml\", \".html\", \".js\"}\n",
    "\n",
    "def convert_repo_to_text(input_dir=\"data/repo_files\", output_file=\"data/combined_repo.txt\"):\n",
    "    all_texts = []\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "\n",
    "            try:\n",
    "                if ext in TEXT_EXTENSIONS:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        content = f.read()\n",
    "                        all_texts.append(f\"\\n\\n===== FILE: {file} =====\\n\\n{content}\")\n",
    "\n",
    "                elif ext == \".ipynb\":  # handle Jupyter notebooks\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        nb = nbformat.read(f, as_version=4)\n",
    "                        cells_text = []\n",
    "                        for cell in nb.cells:\n",
    "                            if cell.cell_type == \"code\" or cell.cell_type == \"markdown\":\n",
    "                                cells_text.append(cell.source)\n",
    "                        all_texts.append(f\"\\n\\n===== FILE: {file} =====\\n\\n\" + \"\\n\\n\".join(cells_text))\n",
    "\n",
    "                else:\n",
    "                    # skip binary files / unsupported files\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Could not read {file_path}: {e}\")\n",
    "\n",
    "    # Write all collected text to a single txt file\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "    print(f\"\\n✔ All files combined into: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab058654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All files combined into: data/combined_repo.txt\n"
     ]
    }
   ],
   "source": [
    "convert_repo_to_text(\"data/repo_files\", \"data/combined_repo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180a923",
   "metadata": {},
   "source": [
    "# Flaten the text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ed4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def flatten_single_file(input_file, output_file=\"data/flattened_repo.txt\"):\n",
    "    \"\"\"\n",
    "    Reads a single text file, flattens the text (removes extra newlines/whitespace),\n",
    "    and writes the flattened text to output_file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Flatten the text\n",
    "        # 1. Replace multiple newlines with one\n",
    "        # 2. Replace multiple spaces/tabs with one space\n",
    "        flat_text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "        flat_text = re.sub(r\"[ \\t]+\", \" \", flat_text)\n",
    "        flat_text = flat_text.strip()\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(flat_text)\n",
    "\n",
    "        print(f\"\\n✔ Flattened file saved as: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error flattening file {input_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8e258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Flattened file saved as: data/flattened_repo.txt\n"
     ]
    }
   ],
   "source": [
    "flatten_single_file(\"data/combined_repo.txt\", \"data/flattened_repo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f7938",
   "metadata": {},
   "source": [
    "# Flaten txt to single line String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b23946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_single_line(input_file=\"data/flattened_repo.txt\"):\n",
    "    \"\"\"\n",
    "    Reads a flattened text file and converts all contents into a single-line string.\n",
    "    Returns the single-line string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Convert to single line: replace newlines with space and remove extra spaces\n",
    "        single_line = \" \".join(text.split())\n",
    "\n",
    "        return single_line\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file {input_file}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81beb7",
   "metadata": {},
   "source": [
    "# Initilize pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee957e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = \"rbi\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05aeb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"\n",
    "    Initialize Pinecone index for hybrid search.\n",
    "    \"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing = [i.name for i in pc.list_indexes()]\n",
    "    \n",
    "    # Delete existing index if dimension mismatch\n",
    "    if INDEX_NAME in existing:\n",
    "        print(f\"Deleting existing index: {INDEX_NAME}\")\n",
    "        pc.delete_index(INDEX_NAME)\n",
    "        time.sleep(5)  # Wait for deletion to complete\n",
    "    \n",
    "    print(f\"Creating Pinecone index: {INDEX_NAME}\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=384,  # all-MiniLM-L6-v2 dimension\n",
    "        metric='dotproduct',\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    \n",
    "    return pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d0777a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def store_terms_to_pinecone(path=\"data/flattened_repo.txt\"):\n",
    "    print(\"Hi\")\n",
    "    combined_text = convert_to_single_line(path)\n",
    "    print(\"\\n✔ Converted text to single line string\")\n",
    "\n",
    "    # Split text into manageable chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(combined_text)\n",
    "    print(f\"  Split into {len(chunks)} text chunks\")\n",
    "\n",
    "    # Initialize Pinecone with better waiting\n",
    "    index = initialize_pinecone()\n",
    "\n",
    "    # Initialize Embeddings and BM25\n",
    "    print(\"\\n  Setting up embedding + sparse encoders...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    \n",
    "    bm25 = BM25Encoder().default()\n",
    "    bm25.fit(chunks)\n",
    "    bm25.dump(BM25_PATH)\n",
    "    print(\"   ✓ BM25 parameters saved\")\n",
    "\n",
    "    # Create retriever and upload vectors\n",
    "    retriever = PineconeHybridSearchRetriever(\n",
    "        embeddings=embeddings,\n",
    "        sparse_encoder=bm25,\n",
    "        index=index\n",
    "    )\n",
    "\n",
    "    print(\"\\n Uploading text chunks to Pinecone...\")\n",
    "    retriever.add_texts(chunks)\n",
    "    print(f\" Successfully stored {len(chunks)} chunks in Pinecone\")\n",
    "    \n",
    "    config = {\n",
    "        \"index_name\": INDEX_NAME,\n",
    "        \"chunk_count\": len(chunks),\n",
    "        \"timestamp\": time.time(),\n",
    "        \"initialized\": True\n",
    "    }\n",
    "\n",
    "    print(config)\n",
    "    return retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5325d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "\n",
      "✔ Converted text to single line string\n",
      "  Split into 37 text chunks\n",
      "Deleting existing index: rbi\n",
      "Creating Pinecone index: rbi\n",
      "\n",
      "  Setting up embedding + sparse encoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 1367.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ BM25 parameters saved\n",
      "\n",
      " Uploading text chunks to Pinecone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully stored 37 chunks in Pinecone\n",
      "{'index_name': 'rbi', 'chunk_count': 37, 'timestamp': 1763363818.2685537, 'initialized': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = await store_terms_to_pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84fa1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What motivated the choice of XGBoost Regressor for modeling in this music popularity prediction project, and how does it compare to other regression algorithms like LightGBM, which is also included in the requirements?\n",
      "2. Can you explain the reasoning behind removing rows with missing values during the preprocessing step, and are there any alternative strategies you would consider for handling missing data in this context?\n",
      "3. How does the project's use of encoding for categorical columns impact the performance of the predictive model, and what encoding strategies were considered or implemented?\n",
      "4. The project splits the data into training and testing sets; what considerations were taken into account when deciding on the split ratio, and how might this affect the model's generalizability?\n",
      "5. The evaluation metrics mentioned include MSE, MAE, and R²; can you discuss why these specific metrics were chosen for assessing the model's performance, and are there any other metrics you would recommend including?\n",
      "6. The project utilizes several libraries, including NumPy, Pandas, and Scikit-learn; how do these libraries contribute to the project's functionality, and are there any specific features or functionalities that made them essential choices?\n",
      "7. The requirements include both XGBoost and LightGBM; what are the trade-offs between these two gradient boosting frameworks, and under what circumstances might one be preferred over the other in this project?\n",
      "8. The project involves calculating performance metrics after training an XGBoost Regressor; can you walk through the process of how these metrics are calculated and how they are used to evaluate the model's performance?\n",
      "9. The documentation mentions the possibility of exploring, experimenting, and enhancing the prediction model; what potential enhancements or modifications could be made to the existing model to improve its predictive accuracy or efficiency?\n",
      "10. The project's data cleaning step involves handling missing values and essential preprocessing; can you elaborate on what specific preprocessing techniques are applied and how they prepare the data for the subsequent modeling steps?\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=1024,\n",
    "        groq_api_key=GROQ_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Create prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a senior software engineer conducting an interview. \n",
    "Use the context below (project code and documentation) to generate interview questions.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Generate 10 meaningful technical questions about the project\n",
    "- Questions must be specific to the code, design, architecture, or data structures\n",
    "- Ask questions like:\n",
    "    - Why did you choose this data structure or algorithm?\n",
    "    - Why this design pattern or component was used?\n",
    "    - Explain the purpose of a specific function/module\n",
    "    - Trade-offs or alternatives in the code\n",
    "- Make the questions suitable for a real-world technical interview\n",
    "- Number the questions from 1 to 10\n",
    "- Only use information present in the context\n",
    "\n",
    "Output format:\n",
    "1. Question 1\n",
    "2. Question 2\n",
    "...\n",
    "10. Question 10\n",
    "\"\"\")\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,          # Pass the chunks from vector DB\n",
    "        \"question\": RunnablePassthrough()  # Not needed, but keeps API consistent\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "question_input = \"Generate interview questions based on this project.\"\n",
    "answer = rag_chain.invoke(question_input)\n",
    "\n",
    "# Get source documents (chunks used for generating questions)\n",
    "docs = retriever.invoke(question_input)\n",
    "\n",
    "# Output the questions\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f5727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
