{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7089c679",
   "metadata": {},
   "source": [
    "# Download valid files from the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96df15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\RBI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import io\n",
    "from docx import Document \n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import List\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import List\n",
    "import nbformat\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_PATH  = \"data/bm25_encoder.json\"  # Path to save/load BM25 encoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb60af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_EXTENSIONS = {\n",
    "    \".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".svg\", \".webp\",\n",
    "    \".ico\", \".tiff\", \".tif\", \".mp3\", \".mp4\", \".wav\", \".avi\",\n",
    "    \".mov\", \".zip\", \".tar\", \".gz\", \".rar\", \".7z\", \".pdf\",\n",
    "    \".exe\", \".dll\", \".so\", \".csv\", \".tsv\"\n",
    "}\n",
    "\n",
    "def download_github_repo(repo_url, save_dir=\"data/repo_files\"):\n",
    "    # remove .git if present\n",
    "    repo_url = repo_url.replace(\".git\", \"\")\n",
    "\n",
    "    match = re.match(r\"https://github.com/([^/]+)/([^/]+)\", repo_url)\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid GitHub URL\")\n",
    "\n",
    "    user, repo = match.groups()\n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def download_recursive(api_path, local_path):\n",
    "        response = requests.get(api_path)\n",
    "        data = response.json()\n",
    "\n",
    "        # handle API errors (rate limit, not found, etc)\n",
    "        if isinstance(data, dict) and \"message\" in data:\n",
    "            print(\"❌ GitHub API Error:\", data[\"message\"])\n",
    "            return\n",
    "\n",
    "        for item in data:\n",
    "            name = item[\"name\"]\n",
    "            file_path = os.path.join(local_path, name)\n",
    "\n",
    "            # skip unwanted folders\n",
    "            if name.lower() in [\"node_modules\", \".git\"]:\n",
    "                continue\n",
    "\n",
    "            if item[\"type\"] == \"dir\":\n",
    "                os.makedirs(file_path, exist_ok=True)\n",
    "                download_recursive(item[\"url\"], file_path)\n",
    "\n",
    "            elif item[\"type\"] == \"file\":\n",
    "                ext = os.path.splitext(name)[1].lower()\n",
    "                if ext in SKIP_EXTENSIONS:\n",
    "                    continue\n",
    "\n",
    "                file_data = requests.get(item[\"download_url\"]).content\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(file_data)\n",
    "\n",
    "                print(\"Downloaded:\", file_path)\n",
    "\n",
    "    download_recursive(api_url, save_dir)\n",
    "    print(\"\\n✔ Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef45606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: data/repo_files\\.gitignore\n",
      "Downloaded: data/repo_files\\Basic_Data_Preprocessing.ipynb\n",
      "Downloaded: data/repo_files\\DataSet_ReadMe.txt\n",
      "Downloaded: data/repo_files\\Detailed End to End Documentation.md\n",
      "Downloaded: data/repo_files\\Micro-Doppler-Based-Target-Classification.pptx\n",
      "Downloaded: data/repo_files\\README.md\n",
      "Downloaded: data/repo_files\\app.py\n",
      "Downloaded: data/repo_files\\main.ipynb\n",
      "Downloaded: data/repo_files\\random_forest_model.pkl\n",
      "\n",
      "✔ Download complete!\n"
     ]
    }
   ],
   "source": [
    "download_github_repo(\"https://github.com/Mageshwaran18/Micro_Doppler_Based_Target_Classification.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc62ce",
   "metadata": {},
   "source": [
    "# Convert those downloaded files to a common txt format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810416ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EXTENSIONS = {\".txt\", \".py\", \".md\", \".json\", \".csv\", \".yaml\", \".yml\", \".html\", \".js\"}\n",
    "\n",
    "def convert_repo_to_text(input_dir=\"data/repo_files\", output_file=\"data/combined_repo.txt\"):\n",
    "    all_texts = []\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "\n",
    "            try:\n",
    "                if ext in TEXT_EXTENSIONS:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        content = f.read()\n",
    "                        all_texts.append(f\"\\n\\n===== FILE: {file} =====\\n\\n{content}\")\n",
    "\n",
    "                elif ext == \".ipynb\":  # handle Jupyter notebooks\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        nb = nbformat.read(f, as_version=4)\n",
    "                        cells_text = []\n",
    "                        for cell in nb.cells:\n",
    "                            if cell.cell_type == \"code\" or cell.cell_type == \"markdown\":\n",
    "                                cells_text.append(cell.source)\n",
    "                        all_texts.append(f\"\\n\\n===== FILE: {file} =====\\n\\n\" + \"\\n\\n\".join(cells_text))\n",
    "\n",
    "                else:\n",
    "                    # skip binary files / unsupported files\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Could not read {file_path}: {e}\")\n",
    "\n",
    "    # Write all collected text to a single txt file\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "    print(f\"\\n✔ All files combined into: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab058654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All files combined into: data/combined_repo.txt\n"
     ]
    }
   ],
   "source": [
    "convert_repo_to_text(\"data/repo_files\", \"data/combined_repo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180a923",
   "metadata": {},
   "source": [
    "# Flaten the text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_single_file(input_file, output_file=\"data/flattened_repo.txt\"):\n",
    "    \"\"\"\n",
    "    Reads a single text file, flattens the text (removes extra newlines/whitespace),\n",
    "    and writes the flattened text to output_file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Flatten the text\n",
    "        # 1. Replace multiple newlines with one\n",
    "        # 2. Replace multiple spaces/tabs with one space\n",
    "        flat_text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "        flat_text = re.sub(r\"[ \\t]+\", \" \", flat_text)\n",
    "        flat_text = flat_text.strip()\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(flat_text)\n",
    "\n",
    "        print(f\"\\n✔ Flattened file saved as: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error flattening file {input_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8e258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Flattened file saved as: data/flattened_repo.txt\n"
     ]
    }
   ],
   "source": [
    "flatten_single_file(\"data/combined_repo.txt\", \"data/flattened_repo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f7938",
   "metadata": {},
   "source": [
    "# Flaten txt to single line String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b23946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_single_line(input_file=\"data/flattened_repo.txt\"):\n",
    "    \"\"\"\n",
    "    Reads a flattened text file and converts all contents into a single-line string.\n",
    "    Returns the single-line string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Convert to single line: replace newlines with space and remove extra spaces\n",
    "        single_line = \" \".join(text.split())\n",
    "\n",
    "        return single_line\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file {input_file}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81beb7",
   "metadata": {},
   "source": [
    "# Initilize pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee957e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = \"rbi\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05aeb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"\n",
    "    Initialize Pinecone index for hybrid search.\n",
    "    \"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing = [i.name for i in pc.list_indexes()]\n",
    "    \n",
    "    # Delete existing index if dimension mismatch\n",
    "    if INDEX_NAME in existing:\n",
    "        print(f\"Deleting existing index: {INDEX_NAME}\")\n",
    "        pc.delete_index(INDEX_NAME)\n",
    "        time.sleep(5)  # Wait for deletion to complete\n",
    "    \n",
    "    print(f\"Creating Pinecone index: {INDEX_NAME}\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=384,  # all-MiniLM-L6-v2 dimension\n",
    "        metric='dotproduct',\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    \n",
    "    return pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0777a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def store_terms_to_pinecone(path=\"data/flattened_repo.txt\"):\n",
    "    combined_text = convert_to_single_line(path)\n",
    "    print(\"\\n✔ Converted text to single line string\")\n",
    "\n",
    "    # Split text into manageable chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(combined_text)\n",
    "    print(f\"  Split into {len(chunks)} text chunks\")\n",
    "\n",
    "    # Remove empty chunks\n",
    "    chunks = [c.strip() for c in chunks if c.strip()]\n",
    "    print(f\"  Filtered to {len(chunks)} non-empty chunks\")\n",
    "\n",
    "    # Initialize Pinecone\n",
    "    index = initialize_pinecone()\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"\\n  Setting up embedding + sparse encoders...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "\n",
    "    # Initialize BM25 BEFORE using it\n",
    "    bm25 = BM25Encoder().default()\n",
    "    bm25.fit(chunks)\n",
    "    bm25.dump(BM25_PATH)\n",
    "    print(\"   ✓ BM25 parameters saved\")\n",
    "\n",
    "    # Create Hybrid retriever\n",
    "    retriever = PineconeHybridSearchRetriever(\n",
    "        embeddings=embeddings,\n",
    "        sparse_encoder=bm25,\n",
    "        index=index\n",
    "    )\n",
    "\n",
    "    # Filter chunks with empty sparse vectors\n",
    "    print(\"\\nFiltering chunks with empty BM25 vectors...\")\n",
    "    valid_chunks = []\n",
    "    for c in chunks:\n",
    "        sparse = bm25.encode_queries([c])[0]  # returns dict: {\"indices\":..., \"values\":...}\n",
    "\n",
    "        # check if sparse vector has non-empty indices\n",
    "        if sparse and len(sparse.get(\"indices\", [])) > 0:\n",
    "            valid_chunks.append(c)\n",
    "\n",
    "    print(f\"  {len(valid_chunks)} chunks have valid sparse representations\")\n",
    "    \n",
    "    if not valid_chunks:\n",
    "        raise ValueError(\"All chunks resulted in empty sparse vectors. Nothing to upload.\")\n",
    "\n",
    "    # Upload only valid chunks\n",
    "    print(\"\\n Uploading valid chunks to Pinecone...\")\n",
    "    retriever.add_texts(valid_chunks)\n",
    "    print(f\" Successfully stored {len(valid_chunks)} chunks in Pinecone\")\n",
    "\n",
    "    # Save config for debugging\n",
    "    config = {\n",
    "        \"index_name\": INDEX_NAME,\n",
    "        \"chunk_count\": len(valid_chunks),\n",
    "        \"timestamp\": time.time(),\n",
    "        \"initialized\": True\n",
    "    }\n",
    "\n",
    "    print(config)\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5325d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Converted text to single line string\n",
      "  Split into 170 text chunks\n",
      "  Filtered to 170 non-empty chunks\n",
      "Deleting existing index: rbi\n",
      "Creating Pinecone index: rbi\n",
      "\n",
      "  Setting up embedding + sparse encoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:00<00:00, 798.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ BM25 parameters saved\n",
      "\n",
      "Filtering chunks with empty BM25 vectors...\n",
      "  169 chunks have valid sparse representations\n",
      "\n",
      " Uploading valid chunks to Pinecone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully stored 169 chunks in Pinecone\n",
      "{'index_name': 'rbi', 'chunk_count': 169, 'timestamp': 1763439949.949413, 'initialized': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = await store_terms_to_pinecone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604bdd5",
   "metadata": {},
   "source": [
    "# Rag chain building and Question generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86156542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM initialization (your config)\n",
    "llm = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=1024,\n",
    "        groq_api_key=GROQ_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fa1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"interview_questions\": [\n",
      "    {\n",
      "      \"question_number\": 1,\n",
      "      \"question\": \"Why was the Continuous Wavelet Transform (CWT) chosen for feature extraction in the model building and training process?\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": 2,\n",
      "      \"question\": \"What was the reasoning behind using a train-test split in the model training process, and what split ratio was used?\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": 3,\n",
      "      \"question\": \"Why was seaborn's heatmap chosen for visualizing the Confusion Matrix, and what benefits does it provide over other visualization methods?\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a senior software engineer conducting an interview. \n",
    "Use the context below (project code and documentation) to generate interview questions.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Generate 3 meaningful technical questions about the project\n",
    "- Questions must be specific to the code, design, architecture, or data structures\n",
    "- A question should consist of only one question. Not a combined form of 2 or 3 question                                          \n",
    "- Don't ask fundamental questions about the project                                        \n",
    "- Ask questions like:\n",
    "    - Why did you choose this data structure or algorithm?\n",
    "    - Why this design pattern or component was used?\n",
    "    - Explain the purpose of a specific function/module\n",
    "    - Trade-offs or alternatives in the code\n",
    "- Make the questions suitable for a real-world technical interview\n",
    "- Number the questions from 1 to 3\n",
    "- Only use information present in the context\n",
    "\n",
    "JSON Output Format (STRICT):\n",
    "\n",
    "{{\n",
    "  \"interview_questions\": [\n",
    "    {{\n",
    "      \"question_number\": <number>,\n",
    "      \"question\": \"<text>\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Additional Output Rules:\n",
    "- Return ONLY JSON (no markdown, no explanation, no comments)\n",
    "- Start question_number at 1 and increment to 5\n",
    "- JSON must be valid and parsable\n",
    "- No trailing commas, no extra text outside JSON\n",
    "\n",
    "\n",
    "Return ONLY valid JSON in the exact format above.\n",
    "                                          \n",
    "\"\"\")\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,          # Pass the chunks from vector DB\n",
    "        \"question\": RunnablePassthrough()  # Not needed, but keeps API consistent\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "question_input = \"Generate interview questions based on this project.\"\n",
    "answer = rag_chain.invoke(question_input)\n",
    "\n",
    "\n",
    "# Store the questions in the json file\n",
    "parsed = json.loads(answer)\n",
    "with open(\"data/questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parsed, f, indent=2)\n",
    "\n",
    "\n",
    "# Get source documents (chunks used for generating questions)\n",
    "docs = retriever.invoke(question_input)\n",
    "\n",
    "# Output the questions\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1fa05",
   "metadata": {},
   "source": [
    "# Asking the question to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_interview_and_save():\n",
    "    \"\"\"Reads questions.json from data/, asks each question, \n",
    "    collects user answers, and writes evaluation.json.\"\"\"\n",
    "    \n",
    "    input_path = os.path.join(\"data\", \"questions.json\")\n",
    "    output_path = os.path.join(\"data\", \"evaluation.json\")\n",
    "\n",
    "    # --- Load questions.json ---\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(\"questions.json not found in /data directory\")\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    questions = data.get(\"interview_questions\", [])\n",
    "    if not questions:\n",
    "        raise ValueError(\"No questions found in questions.json\")\n",
    "\n",
    "    # --- Collect answers from user ---\n",
    "    evaluation_output = {\"evaluation\": []}\n",
    "\n",
    "    print(\"\\nStarting Interview...\\n\")\n",
    "\n",
    "    for q in questions:\n",
    "        print(f\"Q{q['question_number']}: {q['question']}\")\n",
    "        answer = input(\"Your Answer: \")\n",
    "        print()\n",
    "\n",
    "        evaluation_output[\"evaluation\"].append({\n",
    "            \"question_number\": q[\"question_number\"],\n",
    "            \"question\": q[\"question\"],\n",
    "            \"user_answer\": answer\n",
    "        })\n",
    "\n",
    "    # --- Save evaluation.json ---\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evaluation_output, f, indent=4)\n",
    "\n",
    "    print(f\"Interview completed successfully!\")\n",
    "    print(f\"Responses saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9ae34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Interview...\n",
      "\n",
      "Q1: Why was the Continuous Wavelet Transform (CWT) chosen for feature extraction in the model building and training process?\n",
      "\n",
      "Q2: What was the reasoning behind using a train-test split in the model training process, and what split ratio was used?\n",
      "\n",
      "Q3: Why was seaborn's heatmap chosen for visualizing the Confusion Matrix, and what benefits does it provide over other visualization methods?\n",
      "\n",
      "Interview completed successfully!\n",
      "Responses saved to data\\evaluation.json\n"
     ]
    }
   ],
   "source": [
    "conduct_interview_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517dc87",
   "metadata": {},
   "source": [
    "# Evaluate the user's answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95fc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers_with_llm():\n",
    "    input_path = os.path.join(\"data\", \"evaluation.json\")\n",
    "    output_path = os.path.join(\"data\", \"final_output.json\")\n",
    "\n",
    "    # Load evaluation.json\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(\"evaluation.json not found inside /data directory\")\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        evaluation_data = json.load(f)\n",
    "\n",
    "    questions_list = evaluation_data.get(\"evaluation\", [])\n",
    "    if not questions_list:\n",
    "        raise ValueError(\"No evaluation data found in evaluation.json\")\n",
    "\n",
    "    final_output = {\"final_evaluation\": []}\n",
    "\n",
    "    # Process each question through LLM\n",
    "    for item in questions_list:\n",
    "        q_number = item[\"question_number\"]\n",
    "        question = item[\"question\"]\n",
    "        user_answer = item[\"user_answer\"]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an evaluator. \n",
    "Evaluate the user's answer to the interview question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "User Answer:\n",
    "{user_answer}\n",
    "\n",
    "Give marks out of 5. \n",
    "Only respond in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"marks\": <number 0-5>,\n",
    "  \"justification\": \"<short explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        llm_output = response.content.strip()\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(llm_output)\n",
    "        except:\n",
    "            parsed = {\"marks\": 0, \"justification\": \"LLM response not valid JSON.\"}\n",
    "\n",
    "        final_output[\"final_evaluation\"].append({\n",
    "            \"question_number\": q_number,\n",
    "            \"question\": question,\n",
    "            \"user_answer\": user_answer,\n",
    "            \"marks\": parsed.get(\"marks\", 0),\n",
    "            \"justification\": parsed.get(\"justification\", \"\")\n",
    "        })\n",
    "\n",
    "    # Save final_output.json\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "\n",
    "    print(\"✔ Evaluation complete.\")\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd92ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Evaluation complete.\n",
      "Saved to data\\final_output.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_evaluation': [{'question_number': 1,\n",
       "   'question': 'Why was the Continuous Wavelet Transform (CWT) chosen for feature extraction in the model building and training process?',\n",
       "   'user_answer': 'The Continuous Wavelet Transform (CWT) was chosen because it provides a rich time–frequency representation, allowing both temporal and spectral patterns to be captured simultaneously. This makes it ideal for analyzing non-stationary signals and improves the quality of extracted features for the model.',\n",
       "   'marks': 5,\n",
       "   'justification': 'The user provided a clear and accurate explanation of the benefits of using CWT for feature extraction, highlighting its ability to capture both temporal and spectral patterns in non-stationary signals.'},\n",
       "  {'question_number': 2,\n",
       "   'question': 'What was the reasoning behind using a train-test split in the model training process, and what split ratio was used?',\n",
       "   'user_answer': 'A train-test split wasn’t necessary because the model can evaluate itself without holding out any data, and the split ratio used was 50% training and 50% testing.',\n",
       "   'marks': 0,\n",
       "   'justification': \"The user's answer is incorrect because a train-test split is necessary to evaluate a model's performance on unseen data, and the given split ratio of 50% training and 50% testing may not be optimal, but the main issue is the initial statement that a train-test split is not necessary.\"},\n",
       "  {'question_number': 3,\n",
       "   'question': \"Why was seaborn's heatmap chosen for visualizing the Confusion Matrix, and what benefits does it provide over other visualization methods?\",\n",
       "   'user_answer': 'Seaborn’s heatmap was chosen mainly because it automatically improves model accuracy, and it does not provide any advantages over other visualization methods.',\n",
       "   'marks': 0,\n",
       "   'justification': \"The answer is incorrect, as seaborn's heatmap does not automatically improve model accuracy, and it does provide advantages over other visualization methods, such as clear and intuitive representation of the Confusion Matrix.\"}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_answers_with_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c563da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
